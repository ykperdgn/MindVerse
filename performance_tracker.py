#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ“Š MindVerse Blog - GeliÅŸmiÅŸ Performans Tracker
Advanced Performance Tracking & Analytics
"""

import json
import time
import requests
from datetime import datetime, timedelta
from pathlib import Path
from typing import Dict, List, Optional
from collections import defaultdict
import subprocess

class PerformanceTracker:
    def __init__(self):
        self.site_url = "https://www.mindversedaily.com"
        self.data_dir = Path("performance_data")
        self.data_dir.mkdir(exist_ok=True)

        self.metrics_file = self.data_dir / "metrics.json"
        self.reports_dir = Path("reports")
        self.reports_dir.mkdir(exist_ok=True)

        # Performance thresholds
        self.thresholds = {
            "response_time": {"good": 2, "warning": 5, "critical": 10},
            "uptime": {"good": 99.9, "warning": 99.0, "critical": 95.0},
            "content_freshness": {"hours": 24},
            "error_rate": {"good": 0.1, "warning": 1.0, "critical": 5.0}
        }

        print("ğŸ“Š Performance Tracker baÅŸlatÄ±ldÄ±...")

    def collect_performance_metrics(self) -> Dict:
        """Performans metriklerini topla"""
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "site_performance": self.measure_site_performance(),
            "content_metrics": self.analyze_content_performance(),
            "seo_metrics": self.collect_seo_metrics(),
            "user_experience": self.measure_user_experience(),
            "technical_metrics": self.collect_technical_metrics()
        }

        # Metrikleri kaydet
        self.save_metrics(metrics)
        return metrics

    def measure_site_performance(self) -> Dict:
        """Site performansÄ± Ã¶lÃ§Ã¼mÃ¼"""
        performance = {
            "response_times": [],
            "status_codes": [],
            "ssl_info": {},
            "headers": {}
        }

        # Birden fazla istek yaparak ortalama al
        test_urls = [
            self.site_url,
            f"{self.site_url}/astrology/",
            f"{self.site_url}/astrology/haftalik-astroloji-raporu-2025-06-16"
        ]

        for url in test_urls:
            try:
                start_time = time.time()
                response = requests.get(url, timeout=15)
                end_time = time.time()

                response_time = end_time - start_time
                performance["response_times"].append({
                    "url": url,
                    "time": round(response_time, 3),
                    "status": response.status_code
                })

                performance["status_codes"].append(response.status_code)

                # Ä°lk request iÃ§in detay bilgiler
                if url == self.site_url:
                    performance["headers"] = dict(response.headers)
                    performance["ssl_info"] = {
                        "secure": response.url.startswith('https://'),
                        "cert_valid": True  # Basit check
                    }

            except Exception as e:
                performance["response_times"].append({
                    "url": url,
                    "error": str(e),
                    "time": -1
                })

        # Ortalama response time
        valid_times = [r["time"] for r in performance["response_times"] if r["time"] > 0]
        performance["avg_response_time"] = sum(valid_times) / len(valid_times) if valid_times else -1

        # Performance score
        performance["score"] = self.calculate_performance_score(performance)

        return performance

    def analyze_content_performance(self) -> Dict:
        """Ä°Ã§erik performansÄ± analizi"""
        content_dir = Path("src/content")

        metrics = {
            "total_files": 0,
            "recent_files": 0,
            "file_sizes": [],
            "content_types": defaultdict(int),
            "content_quality": {}
        }

        cutoff_time = datetime.now() - timedelta(days=7)

        for content_file in content_dir.rglob("*.md"):
            metrics["total_files"] += 1

            # Dosya boyutu
            file_size = content_file.stat().st_size
            metrics["file_sizes"].append(file_size)

            # Ä°Ã§erik tÃ¼rÃ¼
            path_parts = content_file.parts
            if "astrology" in path_parts:
                metrics["content_types"]["astrology"] += 1
            elif "blog" in path_parts:
                metrics["content_types"]["blog"] += 1
            else:
                metrics["content_types"]["other"] += 1

            # Son dosyalar
            file_time = datetime.fromtimestamp(content_file.stat().st_mtime)
            if file_time > cutoff_time:
                metrics["recent_files"] += 1

        # Ä°Ã§erik kalitesi metrikleri
        metrics["content_quality"] = {
            "avg_file_size": sum(metrics["file_sizes"]) / len(metrics["file_sizes"]) if metrics["file_sizes"] else 0,
            "content_freshness": metrics["recent_files"] / max(metrics["total_files"], 1) * 100,
            "diversity_score": len(metrics["content_types"]) * 10
        }

        return metrics

    def collect_seo_metrics(self) -> Dict:
        """SEO metriklerini topla"""
        seo_metrics = {
            "meta_coverage": 0,
            "heading_structure": {},
            "image_optimization": {},
            "internal_linking": {},
            "page_speed": {}
        }

        try:
            # Ana sayfa SEO kontrolÃ¼
            response = requests.get(self.site_url, timeout=10)
            if response.status_code == 200:
                content = response.text

                # Meta tag kontrolÃ¼
                meta_checks = {
                    "title": "<title>" in content,
                    "description": 'name="description"' in content,
                    "og_title": 'property="og:title"' in content,
                    "og_description": 'property="og:description"' in content,
                    "canonical": 'rel="canonical"' in content
                }

                seo_metrics["meta_coverage"] = sum(meta_checks.values()) / len(meta_checks) * 100

                # Heading yapÄ±sÄ±
                import re
                h1_count = len(re.findall(r'<h1[^>]*>', content))
                h2_count = len(re.findall(r'<h2[^>]*>', content))
                h3_count = len(re.findall(r'<h3[^>]*>', content))

                seo_metrics["heading_structure"] = {
                    "h1_count": h1_count,
                    "h2_count": h2_count,
                    "h3_count": h3_count,
                    "proper_structure": h1_count == 1 and h2_count > 0
                }

                # Image optimization check
                img_tags = re.findall(r'<img[^>]*>', content)
                imgs_with_alt = len(re.findall(r'<img[^>]*alt=["\'][^"\']*["\'][^>]*>', content))

                seo_metrics["image_optimization"] = {
                    "total_images": len(img_tags),
                    "images_with_alt": imgs_with_alt,
                    "alt_coverage": imgs_with_alt / max(len(img_tags), 1) * 100
                }

        except Exception as e:
            seo_metrics["error"] = str(e)

        return seo_metrics

    def measure_user_experience(self) -> Dict:
        """KullanÄ±cÄ± deneyimi Ã¶lÃ§Ã¼mÃ¼"""
        ux_metrics = {
            "mobile_friendly": True,  # VarsayÄ±lan
            "accessibility_score": 0,
            "loading_experience": {},
            "interaction_readiness": {}
        }

        # Lighthouse-style metrikleri simÃ¼le et
        try:
            start_time = time.time()
            response = requests.get(self.site_url, timeout=15)
            load_time = time.time() - start_time

            ux_metrics["loading_experience"] = {
                "first_contentful_paint": round(load_time * 0.6, 2),
                "largest_contentful_paint": round(load_time * 0.8, 2),
                "cumulative_layout_shift": round(0.1, 3),  # Simulated
                "first_input_delay": round(100, 0)  # ms
            }

            # UX score hesapla
            lcp = ux_metrics["loading_experience"]["largest_contentful_paint"]
            if lcp < 2.5:
                ux_score = 90
            elif lcp < 4.0:
                ux_score = 70
            else:
                ux_score = 50

            ux_metrics["overall_score"] = ux_score

        except Exception as e:
            ux_metrics["error"] = str(e)

        return ux_metrics

    def collect_technical_metrics(self) -> Dict:
        """Teknik metrikler"""
        tech_metrics = {
            "build_performance": {},
            "deployment_status": {},
            "dependencies": {},
            "code_quality": {}
        }

        # Build performance
        try:
            start_time = time.time()
            result = subprocess.run(
                ["npm", "run", "build"],
                capture_output=True,
                text=True,
                timeout=180
            )
            build_time = time.time() - start_time

            tech_metrics["build_performance"] = {
                "build_time": round(build_time, 2),
                "success": result.returncode == 0,
                "output_size": len(result.stdout) + len(result.stderr)
            }

        except Exception as e:
            tech_metrics["build_performance"] = {"error": str(e)}

        # Package.json analizi
        try:
            with open("package.json", "r") as f:
                package_data = json.load(f)

            tech_metrics["dependencies"] = {
                "total_deps": len(package_data.get("dependencies", {})),
                "dev_deps": len(package_data.get("devDependencies", {})),
                "has_lockfile": Path("package-lock.json").exists()
            }

        except Exception as e:
            tech_metrics["dependencies"] = {"error": str(e)}

        return tech_metrics

    def calculate_performance_score(self, performance_data: Dict) -> int:
        """Performans skoru hesapla"""
        score = 100

        # Response time penalty
        avg_time = performance_data.get("avg_response_time", 0)
        if avg_time > self.thresholds["response_time"]["critical"]:
            score -= 40
        elif avg_time > self.thresholds["response_time"]["warning"]:
            score -= 20
        elif avg_time > self.thresholds["response_time"]["good"]:
            score -= 10

        # Status code penalty
        status_codes = performance_data.get("status_codes", [])
        error_count = sum(1 for code in status_codes if code >= 400)
        if error_count > 0:
            score -= error_count * 15

        # SSL bonus
        if performance_data.get("ssl_info", {}).get("secure"):
            score += 5

        return max(0, min(100, score))

    def save_metrics(self, metrics: Dict):
        """Metrikleri kaydet"""
        historical_data = []

        if self.metrics_file.exists():
            try:
                with open(self.metrics_file, 'r') as f:
                    historical_data = json.load(f)
            except:
                historical_data = []

        historical_data.append(metrics)

        # Son 1000 kayÄ±t tut
        if len(historical_data) > 1000:
            historical_data = historical_data[-1000:]

        with open(self.metrics_file, 'w') as f:
            json.dump(historical_data, f, indent=2, ensure_ascii=False)

    def generate_performance_report(self, days: int = 7) -> str:
        """Performans raporu oluÅŸtur"""
        if not self.metrics_file.exists():
            return "Performans verisi bulunamadÄ±"

        try:
            with open(self.metrics_file, 'r') as f:
                data = json.load(f)
        except:
            return "Veri okuma hatasÄ±"

        if not data:
            return "Veri boÅŸ"

        # Son N gÃ¼n verilerini filtrele
        cutoff_time = datetime.now() - timedelta(days=days)
        recent_data = []

        for entry in data:
            try:
                entry_time = datetime.fromisoformat(entry['timestamp'].replace('Z', '+00:00'))
                if entry_time > cutoff_time:
                    recent_data.append(entry)
            except:
                continue

        if not recent_data:
            return f"Son {days} gÃ¼n veri yok"

        # Analiz
        response_times = []
        performance_scores = []
        uptime_data = []

        for entry in recent_data:
            site_perf = entry.get('site_performance', {})

            if 'avg_response_time' in site_perf and site_perf['avg_response_time'] > 0:
                response_times.append(site_perf['avg_response_time'])

            if 'score' in site_perf:
                performance_scores.append(site_perf['score'])

            # Uptime calculation
            status_codes = site_perf.get('status_codes', [])
            if status_codes:
                successful_requests = sum(1 for code in status_codes if 200 <= code < 400)
                uptime_data.append(successful_requests / len(status_codes) * 100)

        # Ä°statistikler
        avg_response = sum(response_times) / len(response_times) if response_times else 0
        avg_performance = sum(performance_scores) / len(performance_scores) if performance_scores else 0
        avg_uptime = sum(uptime_data) / len(uptime_data) if uptime_data else 0

        # Trend analizi
        if len(performance_scores) >= 2:
            recent_avg = sum(performance_scores[-5:]) / min(5, len(performance_scores))
            older_avg = sum(performance_scores[:-5]) / max(1, len(performance_scores) - 5)
            trend = "ğŸ“ˆ Ä°yileÅŸiyor" if recent_avg > older_avg else "ğŸ“‰ KÃ¶tÃ¼leÅŸiyor" if recent_avg < older_avg else "â¡ï¸ Stabil"
        else:
            trend = "â¡ï¸ Yeterli veri yok"

        # Rapor oluÅŸtur
        timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
        report_file = self.reports_dir / f"performance_report_{timestamp}.md"

        report_content = f"""# ğŸ“Š Performans Analiz Raporu
**Tarih:** {datetime.now().strftime("%d %B %Y, %H:%M")}
**Periode:** Son {days} gÃ¼n
**Veri NoktasÄ±:** {len(recent_data)} Ã¶lÃ§Ã¼m

---

## ğŸ¯ Ã–zet Metrikler

### Performans Skoru
- **Ortalama:** {avg_performance:.1f}/100
- **Trend:** {trend}
- **Durum:** {'ğŸŸ¢ Ä°yi' if avg_performance >= 80 else 'ğŸŸ¡ Orta' if avg_performance >= 60 else 'ğŸ”´ KÃ¶tÃ¼'}

### Site HÄ±zÄ±
- **Ortalama Response Time:** {avg_response:.2f} saniye
- **Durum:** {'ğŸŸ¢ HÄ±zlÄ±' if avg_response < 2 else 'ğŸŸ¡ Orta' if avg_response < 5 else 'ğŸ”´ YavaÅŸ'}

### Uptime
- **Ortalama Uptime:** %{avg_uptime:.2f}
- **Durum:** {'ğŸŸ¢ MÃ¼kemmel' if avg_uptime >= 99.9 else 'ğŸŸ¡ Ä°yi' if avg_uptime >= 99 else 'ğŸ”´ Sorunlu'}

---

## ğŸ“ˆ DetaylÄ± Analiz

### Response Time DaÄŸÄ±lÄ±mÄ±
"""

        if response_times:
            min_time = min(response_times)
            max_time = max(response_times)
            report_content += f"""
- **En HÄ±zlÄ±:** {min_time:.2f}s
- **En YavaÅŸ:** {max_time:.2f}s
- **Medyan:** {sorted(response_times)[len(response_times)//2]:.2f}s
"""

        report_content += f"""

### Son 5 Ã–lÃ§Ã¼m
"""

        # Son 5 Ã¶lÃ§Ã¼mÃ¼ gÃ¶ster
        for i, entry in enumerate(recent_data[-5:], 1):
            timestamp = entry['timestamp'][:19]
            site_perf = entry.get('site_performance', {})
            score = site_perf.get('score', 0)
            response_time = site_perf.get('avg_response_time', 0)

            status_emoji = "ğŸŸ¢" if score >= 80 else "ğŸŸ¡" if score >= 60 else "ğŸ”´"
            report_content += f"**{i}.** {timestamp} - {status_emoji} Score: {score}/100, Time: {response_time:.2f}s\n"

        report_content += f"""

---

## ğŸ” Ä°Ã§erik PerformansÄ±

### Ä°Ã§erik Metrikleri (Son Ã–lÃ§Ã¼m)
"""

        if recent_data:
            content_metrics = recent_data[-1].get('content_metrics', {})
            quality = content_metrics.get('content_quality', {})

            report_content += f"""
- **Toplam Dosya:** {content_metrics.get('total_files', 0)}
- **Son Hafta Yeni:** {content_metrics.get('recent_files', 0)}
- **Ä°Ã§erik Tazelik:** %{quality.get('content_freshness', 0):.1f}
- **Ã‡eÅŸitlilik Skoru:** {quality.get('diversity_score', 0)}
"""

        report_content += f"""

---

## ğŸ¯ SEO PerformansÄ±

### SEO Metrikleri (Son Ã–lÃ§Ã¼m)
"""

        if recent_data:
            seo_metrics = recent_data[-1].get('seo_metrics', {})

            report_content += f"""
- **Meta Tag KapsamÄ±:** %{seo_metrics.get('meta_coverage', 0):.1f}
- **GÃ¶rsel Alt Text:** %{seo_metrics.get('image_optimization', {}).get('alt_coverage', 0):.1f}
- **Heading YapÄ±sÄ±:** {'âœ… Uygun' if seo_metrics.get('heading_structure', {}).get('proper_structure') else 'âŒ DÃ¼zeltilmeli'}
"""

        report_content += f"""

---

## ğŸ’¡ Ã–neriler

### Performans Ä°yileÅŸtirme
"""

        # Performansa gÃ¶re Ã¶neriler
        if avg_response > 5:
            report_content += "- ğŸ”´ **Kritik:** Response time Ã§ok yÃ¼ksek - CDN ve caching optimizasyonu gerekli\n"
        elif avg_response > 2:
            report_content += "- ğŸŸ¡ **UyarÄ±:** Response time iyileÅŸtirilebilir - Image optimization Ã¶nerili\n"
        else:
            report_content += "- ğŸŸ¢ **Ä°yi:** Response time kabul edilebilir seviyede\n"

        if avg_uptime < 99:
            report_content += "- ğŸ”´ **Kritik:** Uptime dÃ¼ÅŸÃ¼k - hosting ve monitoring kontrolÃ¼ gerekli\n"
        elif avg_uptime < 99.9:
            report_content += "- ğŸŸ¡ **UyarÄ±:** Uptime iyileÅŸtirilebilir - redundancy eklenebilir\n"
        else:
            report_content += "- ğŸŸ¢ **MÃ¼kemmel:** Uptime hedef seviyede\n"

        report_content += f"""

### Sonraki AdÄ±mlar
1. **Performans Ä°zleme:** GÃ¼nlÃ¼k otomatik kontroller
2. **Trend Analizi:** HaftalÄ±k performans deÄŸerlendirmesi
3. **Optimizasyon:** Belirlenen sorun alanlarÄ±nda iyileÅŸtirme
4. **Monitoring Alerts:** Kritik threshold'lar iÃ§in uyarÄ± sistemi

---

**Rapor OluÅŸturma:** {datetime.now().strftime("%d %B %Y, %H:%M:%S")}
**Sonraki Rapor:** 1 hafta sonra Ã¶nerilir

*Bu rapor Performance Tracker tarafÄ±ndan otomatik oluÅŸturulmuÅŸtur.*
"""

        # Raporu kaydet
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write(report_content)

        print(f"ğŸ“Š Performans raporu oluÅŸturuldu: {report_file}")
        return str(report_file)

    def run_continuous_monitoring(self, interval_minutes: int = 30):
        """SÃ¼rekli izleme"""
        print(f"ğŸ”„ SÃ¼rekli performans izleme baÅŸlatÄ±ldÄ± (her {interval_minutes} dakika)")
        print("CTRL+C ile durdurun")

        try:
            while True:
                print(f"\nğŸ“Š {datetime.now().strftime('%H:%M:%S')} - Performans Ã¶lÃ§Ã¼mÃ¼...")
                metrics = self.collect_performance_metrics()

                # AnlÄ±k sonuÃ§larÄ± gÃ¶ster
                site_perf = metrics.get('site_performance', {})
                score = site_perf.get('score', 0)
                response_time = site_perf.get('avg_response_time', 0)

                status_emoji = "ğŸŸ¢" if score >= 80 else "ğŸŸ¡" if score >= 60 else "ğŸ”´"
                print(f"   {status_emoji} Score: {score}/100, Response: {response_time:.2f}s")

                # Kritik durumlar iÃ§in uyarÄ±
                if score < 50:
                    print("   ğŸš¨ KRITIK: Performans Ã§ok dÃ¼ÅŸÃ¼k!")
                elif response_time > 10:
                    print("   âš ï¸ UYARI: Response time Ã§ok yÃ¼ksek!")

                # Bekleme
                time.sleep(interval_minutes * 60)

        except KeyboardInterrupt:
            print("\nâ¹ï¸ SÃ¼rekli izleme durduruldu")

    def run_interactive_mode(self):
        """Ä°nteraktif performans tracker"""
        print("\nğŸ“Š Performans Tracker")
        print("=" * 30)

        while True:
            print("\nğŸ”§ Performans SeÃ§enekleri:")
            print("1. ğŸ“Š AnlÄ±k Performans Ã–lÃ§Ã¼mÃ¼")
            print("2. ğŸ“ˆ Performans Raporu (7 gÃ¼n)")
            print("3. ğŸ“ˆ Performans Raporu (30 gÃ¼n)")
            print("4. ğŸ”„ SÃ¼rekli Ä°zleme BaÅŸlat")
            print("5. ğŸ“‹ GeÃ§miÅŸ Veriler")
            print("6. ğŸ¯ Performans Skoru DetayÄ±")
            print("7. âš™ï¸ Threshold AyarlarÄ±")
            print("8. ğŸ“¤ Rapor DÄ±ÅŸa Aktar")
            print("9. Ã‡Ä±kÄ±ÅŸ")

            choice = input("\nSeÃ§iminiz (1-9): ").strip()

            if choice == "1":
                print("ğŸ“Š Performans Ã¶lÃ§Ã¼mÃ¼ yapÄ±lÄ±yor...")
                metrics = self.collect_performance_metrics()

                site_perf = metrics.get('site_performance', {})
                content_perf = metrics.get('content_metrics', {})
                seo_perf = metrics.get('seo_metrics', {})

                print(f"\nğŸ¯ Performans SonuÃ§larÄ±:")
                print(f"   Site Skoru: {site_perf.get('score', 0)}/100")
                print(f"   Response Time: {site_perf.get('avg_response_time', 0):.2f}s")
                print(f"   Ä°Ã§erik DosyasÄ±: {content_perf.get('total_files', 0)}")
                print(f"   SEO KapsamÄ±: %{seo_perf.get('meta_coverage', 0):.1f}")

            elif choice == "2":
                report_file = self.generate_performance_report(7)
                print(f"ğŸ“ˆ 7 gÃ¼nlÃ¼k rapor oluÅŸturuldu: {report_file}")

            elif choice == "3":
                report_file = self.generate_performance_report(30)
                print(f"ğŸ“ˆ 30 gÃ¼nlÃ¼k rapor oluÅŸturuldu: {report_file}")

            elif choice == "4":
                interval = input("Ä°zleme aralÄ±ÄŸÄ± (dakika, varsayÄ±lan 30): ").strip()
                interval = int(interval) if interval.isdigit() else 30
                self.run_continuous_monitoring(interval)

            elif choice == "5":
                if self.metrics_file.exists():
                    with open(self.metrics_file, 'r') as f:
                        data = json.load(f)
                    print(f"ğŸ“‹ Toplam veri noktasÄ±: {len(data)}")
                    if data:
                        print(f"   Ä°lk kayÄ±t: {data[0]['timestamp'][:19]}")
                        print(f"   Son kayÄ±t: {data[-1]['timestamp'][:19]}")
                else:
                    print("ğŸ“‹ HenÃ¼z veri kaydÄ± yok")

            elif choice == "6":
                print("ğŸ¯ Performans Skoru Kriterleri:")
                print("   100-80: ğŸŸ¢ MÃ¼kemmel")
                print("   79-60:  ğŸŸ¡ Ä°yi")
                print("   59-40:  ğŸŸ  Orta")
                print("   39-0:   ğŸ”´ KÃ¶tÃ¼")
                print("\n   FaktÃ¶rler:")
                print("   â€¢ Response Time (en Ã¶nemli)")
                print("   â€¢ HTTP Status Codes")
                print("   â€¢ SSL Security")
                print("   â€¢ Error Rate")

            elif choice == "7":
                print("âš™ï¸ Mevcut Threshold'lar:")
                for metric, values in self.thresholds.items():
                    print(f"   {metric}: {values}")

            elif choice == "8":
                print("ğŸ“¤ Rapor dÄ±ÅŸa aktarma Ã¶zellikleri:")
                print("   â€¢ Markdown raporlarÄ± reports/ klasÃ¶rÃ¼nde")
                print("   â€¢ JSON verileri performance_data/ klasÃ¶rÃ¼nde")
                print("   â€¢ GeÃ§miÅŸ tÃ¼m veriler metrics.json'da")

            elif choice == "9":
                print("ğŸ‘‹ Performance Tracker kapatÄ±lÄ±yor...")
                break

            else:
                print("âŒ GeÃ§ersiz seÃ§im (1-9 arasÄ±)")

def main():
    """Ana fonksiyon"""
    tracker = PerformanceTracker()
    tracker.run_interactive_mode()

if __name__ == "__main__":
    main()
